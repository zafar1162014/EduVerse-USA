{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EduVerse USA Chatbot — Named Entity Recognition\n",
        "\n",
        "## NLP Pipeline Module 3\n",
        "\n",
        "This notebook extracts academic entities from user queries.\n",
        "\n",
        "### Entity Types\n",
        "- **Universities**: MIT, Stanford, USC\n",
        "- **Programs**: MS in Computer Science, PhD in ML\n",
        "- **Locations**: US states (CA, NY, TX)\n",
        "- **Tests**: GRE, TOEFL, IELTS\n",
        "- **Deadlines**: Fall 2026, Dec 15\n",
        "- **Scores**: GPA 3.8, TOEFL 105\n",
        "\n",
        "### Approach\n",
        "Rule-based extraction with regex patterns and gazetteers, plus spaCy integration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install spacy pandas matplotlib -q\n",
        "!python -m spacy download en_core_web_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(\"Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Entity Gazetteers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# University names\n",
        "UNIVERSITIES = [\n",
        "    \"mit\", \"stanford\", \"harvard\", \"carnegie mellon\", \"cmu\",\n",
        "    \"uc berkeley\", \"berkeley\", \"usc\", \"nyu\", \"columbia\",\n",
        "    \"cornell\", \"princeton\", \"yale\", \"ucla\", \"caltech\"\n",
        "]\n",
        "\n",
        "# US states\n",
        "US_STATES = {\n",
        "    \"california\": \"CA\", \"ca\": \"CA\",\n",
        "    \"new york\": \"NY\", \"ny\": \"NY\",\n",
        "    \"texas\": \"TX\", \"tx\": \"TX\",\n",
        "    \"massachusetts\": \"MA\", \"ma\": \"MA\"\n",
        "}\n",
        "\n",
        "# Standardized tests\n",
        "TESTS = [\"gre\", \"toefl\", \"ielts\", \"gmat\"]\n",
        "\n",
        "print(f\"Universities: {len(UNIVERSITIES)}\")\n",
        "print(f\"States: {len(US_STATES)}\")\n",
        "print(f\"Tests: {len(TESTS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Regex Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Program patterns\n",
        "PROGRAM_PATTERNS = [\n",
        "    r'\\b(ms|master\\'?s?)\\s+(in\\s+)?([a-z\\s]+)',\n",
        "    r'\\b(phd|doctorate)\\s+(in\\s+)?([a-z\\s]+)',\n",
        "    r'\\bcomputer\\s+science\\b',\n",
        "    r'\\bdata\\s+science\\b',\n",
        "    r'\\bmachine\\s+learning\\b'\n",
        "]\n",
        "\n",
        "# Deadline patterns\n",
        "DEADLINE_PATTERNS = [\n",
        "    r'\\b(fall|spring)\\s+20\\d{2}\\b',\n",
        "    r'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\\s+\\d{1,2}\\b'\n",
        "]\n",
        "\n",
        "# Score patterns\n",
        "SCORE_PATTERNS = {\n",
        "    'gpa': r'\\bgpa\\s*[:\\-]?\\s*(\\d\\.\\d{1,2})\\b',\n",
        "    'toefl': r'\\btoefl\\s*[:\\-]?\\s*(\\d{2,3})\\b',\n",
        "    'ielts': r'\\bielts\\s*[:\\-]?\\s*(\\d(?:\\.\\d)?)\\b',\n",
        "    'gre': r'\\bgre\\s*[:\\-]?\\s*(\\d{3})\\b'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Entity Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Entities:\n",
        "    universities: List[str]\n",
        "    programs: List[str]\n",
        "    locations: List[str]\n",
        "    tests: List[str]\n",
        "    deadlines: List[str]\n",
        "    scores: Dict[str, str]\n",
        "\n",
        "\n",
        "def extract_entities(text: str) -> Entities:\n",
        "    \"\"\"Extract academic entities from text.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    \n",
        "    # Universities\n",
        "    universities = [u.title() for u in UNIVERSITIES if u in text_lower]\n",
        "    universities = list(set(universities))\n",
        "    \n",
        "    # Programs\n",
        "    programs = []\n",
        "    for pattern in PROGRAM_PATTERNS:\n",
        "        for m in re.finditer(pattern, text_lower):\n",
        "            programs.append(m.group(0).strip())\n",
        "    programs = list(set(programs))\n",
        "    \n",
        "    # Locations\n",
        "    locations = []\n",
        "    for state, abbrev in US_STATES.items():\n",
        "        if state in text_lower:\n",
        "            locations.append(abbrev)\n",
        "    locations = list(set(locations))\n",
        "    \n",
        "    # Tests\n",
        "    tests = [t.upper() for t in TESTS if t in text_lower]\n",
        "    tests = list(set(tests))\n",
        "    \n",
        "    # Deadlines\n",
        "    deadlines = []\n",
        "    for pattern in DEADLINE_PATTERNS:\n",
        "        for m in re.finditer(pattern, text_lower):\n",
        "            deadlines.append(m.group(0))\n",
        "    \n",
        "    # Scores\n",
        "    scores = {}\n",
        "    for score_type, pattern in SCORE_PATTERNS.items():\n",
        "        m = re.search(pattern, text_lower)\n",
        "        if m:\n",
        "            scores[score_type] = m.group(1)\n",
        "    \n",
        "    return Entities(universities, programs, locations, tests, deadlines, scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Test Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_queries = [\n",
        "    \"I want MS in Computer Science at Stanford for Fall 2026. GPA: 3.8\",\n",
        "    \"What's the TOEFL requirement for MIT? I scored 105.\",\n",
        "    \"PhD programs in Machine Learning at UC Berkeley, California.\",\n",
        "    \"USC deadline is Dec 15. I need GRE 320 and IELTS 7.0\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    ent = extract_entities(query)\n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"  Universities: {ent.universities}\")\n",
        "    print(f\"  Programs: {ent.programs}\")\n",
        "    print(f\"  Tests: {ent.tests}\")\n",
        "    print(f\"  Scores: {ent.scores}\")\n",
        "    print(f\"  Deadlines: {ent.deadlines}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. spaCy NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample = \"I want to study at MIT in Massachusetts starting Fall 2026.\"\n",
        "doc = nlp(sample)\n",
        "\n",
        "print(f\"Text: {sample}\\n\")\n",
        "print(\"spaCy Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"  {ent.text} → {ent.label_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Combined Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combined_ner(text):\n",
        "    \"\"\"Combine rule-based and spaCy NER.\"\"\"\n",
        "    # Rule-based\n",
        "    rule_ent = extract_entities(text)\n",
        "    \n",
        "    # spaCy\n",
        "    doc = nlp(text)\n",
        "    spacy_ent = {}\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ not in spacy_ent:\n",
        "            spacy_ent[ent.label_] = []\n",
        "        spacy_ent[ent.label_].append(ent.text)\n",
        "    \n",
        "    return {'rule_based': rule_ent, 'spacy': spacy_ent}\n",
        "\n",
        "# Test\n",
        "result = combined_ner(\"John wants to apply to Stanford in California for PhD in AI. GRE: 325\")\n",
        "\n",
        "print(\"Rule-based:\")\n",
        "print(f\"  Universities: {result['rule_based'].universities}\")\n",
        "print(f\"  Programs: {result['rule_based'].programs}\")\n",
        "print(f\"  Scores: {result['rule_based'].scores}\")\n",
        "\n",
        "print(\"\\nspaCy:\")\n",
        "for label, entities in result['spacy'].items():\n",
        "    print(f\"  {label}: {entities}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Entity Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "entity_counts = defaultdict(int)\n",
        "\n",
        "for query in test_queries:\n",
        "    ent = extract_entities(query)\n",
        "    if ent.universities: entity_counts['Universities'] += len(ent.universities)\n",
        "    if ent.programs: entity_counts['Programs'] += len(ent.programs)\n",
        "    if ent.tests: entity_counts['Tests'] += len(ent.tests)\n",
        "    if ent.scores: entity_counts['Scores'] += len(ent.scores)\n",
        "    if ent.deadlines: entity_counts['Deadlines'] += len(ent.deadlines)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6', '#f39c12']\n",
        "plt.bar(entity_counts.keys(), entity_counts.values(), color=colors)\n",
        "plt.title('Entity Distribution', fontsize=14)\n",
        "plt.xlabel('Entity Type')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Summary\n",
        "\n",
        "### NER Approach\n",
        "- **Rule-based**: Domain-specific entities (universities, programs, tests, scores)\n",
        "- **spaCy**: General entities (names, dates, locations)\n",
        "- **Combined**: Best of both approaches\n",
        "\n",
        "### Key Takeaways\n",
        "1. Gazetteers effectively capture university names\n",
        "2. Regex patterns handle structured entities (scores, dates)\n",
        "3. spaCy complements with general entity recognition"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}