{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EduVerse USA Chatbot — Text Preprocessing\n",
        "\n",
        "## NLP Pipeline Module 1\n",
        "\n",
        "This notebook demonstrates the text preprocessing pipeline for the chatbot.\n",
        "\n",
        "### Pipeline Steps\n",
        "1. Text normalization (lowercase, remove URLs/emails)\n",
        "2. Tokenization using NLTK\n",
        "3. Stopword removal\n",
        "4. Lemmatization\n",
        "\n",
        "### Output\n",
        "Clean token sequences ready for intent classification and NER."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nltk pandas matplotlib -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "for resource in ['punkt', 'stopwords', 'wordnet', 'omw-1.4']:\n",
        "    nltk.download(resource, quiet=True)\n",
        "\n",
        "print(\"Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample user queries for U.S. study-abroad guidance\n",
        "sample_queries = [\n",
        "    \"What documents are required for MS admissions in the USA?\",\n",
        "    \"I want to apply to Stanford for Fall 2026. My GPA is 3.8 and TOEFL score is 105.\",\n",
        "    \"How do I write a strong Statement of Purpose for computer science programs?\",\n",
        "    \"Are there any scholarships available for international students at MIT?\",\n",
        "    \"GRE prep tips??? I need 320+ score in 2 months!!\",\n",
        "    \"What's the difference between TOEFL and IELTS?\",\n",
        "    \"Can you review my SOP draft? Here's the link: https://docs.google.com/example\",\n",
        "    \"Application deadline for USC MS in CS program - when is it?\",\n",
        "    \"PhD in Machine Learning - funding options?\",\n",
        "    \"How many recommendation letters do I need?\"\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({'query': sample_queries})\n",
        "print(f\"Dataset: {len(df)} queries\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"Clean and normalize raw text.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)  # Remove URLs\n",
        "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', text)  # Remove emails\n",
        "    text = re.sub(r'[^a-z0-9\\s\\.\\,\\?]', ' ', text)  # Keep alphanumeric + basic punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Collapse whitespace\n",
        "    return text\n",
        "\n",
        "df['normalized'] = df['query'].apply(normalize_text)\n",
        "\n",
        "# Show examples\n",
        "for i in range(3):\n",
        "    print(f\"Original:   {df['query'].iloc[i]}\")\n",
        "    print(f\"Normalized: {df['normalized'].iloc[i]}\")\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"Split text into word tokens.\"\"\"\n",
        "    return word_tokenize(text)\n",
        "\n",
        "df['tokens'] = df['normalized'].apply(tokenize)\n",
        "\n",
        "# Show example\n",
        "print(f\"Text:   {df['normalized'].iloc[0]}\")\n",
        "print(f\"Tokens: {df['tokens'].iloc[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"Filter out common stopwords.\"\"\"\n",
        "    return [t for t in tokens if t not in stop_words and len(t) > 1]\n",
        "\n",
        "df['filtered'] = df['tokens'].apply(remove_stopwords)\n",
        "\n",
        "# Compare\n",
        "print(f\"Before: {df['tokens'].iloc[0]}\")\n",
        "print(f\"After:  {df['filtered'].iloc[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    \"\"\"Reduce words to base form.\"\"\"\n",
        "    return [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "df['lemmatized'] = df['filtered'].apply(lemmatize)\n",
        "\n",
        "# Show examples\n",
        "examples = ['universities', 'applications', 'requirements', 'studies']\n",
        "for word in examples:\n",
        "    print(f\"{word} → {lemmatizer.lemmatize(word)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Complete Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    \"\"\"Complete preprocessing pipeline.\"\"\"\n",
        "    text = normalize_text(text)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = lemmatize(tokens)\n",
        "    return tokens\n",
        "\n",
        "# Test\n",
        "test = \"I want to apply for MS admissions at Stanford University for Fall 2026.\"\n",
        "print(f\"Input:  {test}\")\n",
        "print(f\"Output: {preprocess(test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Token Frequency Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all tokens\n",
        "all_tokens = [t for tokens in df['lemmatized'] for t in tokens]\n",
        "token_freq = Counter(all_tokens).most_common(15)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar([t[0] for t in token_freq], [t[1] for t in token_freq], color='steelblue')\n",
        "plt.xlabel('Token')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 15 Tokens After Preprocessing')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Summary\n",
        "\n",
        "### Pipeline Output\n",
        "Raw text → Normalized → Tokenized → Stopwords removed → Lemmatized\n",
        "\n",
        "### Key Takeaways\n",
        "- **Normalization** removes noise while preserving meaningful content\n",
        "- **Tokenization** breaks text into analyzable units\n",
        "- **Stopword removal** focuses on content-bearing words\n",
        "- **Lemmatization** reduces vocabulary size while maintaining meaning\n",
        "\n",
        "These preprocessed tokens feed into intent classification and NER modules."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}